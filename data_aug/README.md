# Linguistic adversity dependency
### WN

- NLTK, with following packages downloaded:
-- averaged_perceptron_tagger
-- punkt
-- stopwords
-- universal_tagset
-- wordnet
- Numpy
- kenLM, with pre-built n-gram language model
- Stanford-ner


### CFit
Based on idea of [Counter-fitting](https://arxiv.org/abs/1603.00892)

- NLTK
- Numpy
- kenLM
- Stanford-ner
- pre-trained counter-fitting dictionary


### ERG
Based on English Resource Grammar (ERG) system, and ACE.
Dependencies:
- ERG ACE


---

For more details of the algorithm, please refer to original paper:

Li, Yitong , Trevor Cohn and Timothy Baldwin (2017) Robust Training under Linguistic Adversity, In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2017), Valencia, Spain.

